---
title: "US Income"
author: "Olivia Chang"
date: 2023-04-10
output:
  github_document:
    toc: true
prerequisites:
  - e-stat09-bootstrap
---

*Purpose*: We've been learning how to quantify uncertainty in estimates through the exercises; now its time to put those skills to use studying real data. In this challenge we'll use concepts like confidence intervals to help us make sense of census data.

*Reading*:
- [Using ACS Estimates and Margin of Error](https://www.census.gov/data/academy/webinars/2020/calculating-margins-of-error-acs.html) (Optional, see the PDF on the page)
- [Patterns and Causes of Uncertainty in the American Community Survey](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNq56Y2IzszmGgIGH4JAPzQN0) (Optional, particularly the *Uncertainty in surveys* section under the Introduction.)

<!-- include-rubric -->
# Grading Rubric
<!-- -------------------------------------------------- -->

Unlike exercises, **challenges will be graded**. The following rubrics define how you will be graded, both on an individual and team basis.

## Individual
<!-- ------------------------- -->

| Category | Needs Improvement | Satisfactory |
|----------|----------------|--------------|
| Effort | Some task __q__'s left unattempted | All task __q__'s attempted |
| Observed | Did not document observations, or observations incorrect | Documented correct observations based on analysis |
| Supported | Some observations not clearly supported by analysis | All observations clearly supported by analysis (table, graph, etc.) |
| Assessed | Observations include claims not supported by the data, or reflect a level of certainty not warranted by the data | Observations are appropriately qualified by the quality & relevance of the data and (in)conclusiveness of the support |
| Specified | Uses the phrase "more data are necessary" without clarification | Any statement that "more data are necessary" specifies which *specific* data are needed to answer what *specific* question |
| Code Styled | Violations of the [style guide](https://style.tidyverse.org/) hinder readability | Code sufficiently close to the [style guide](https://style.tidyverse.org/) |

## Submission
<!-- ------------------------- -->

Make sure to commit both the challenge report (`report.md` file) and supporting files (`report_files/` folder) when you are done! Then submit a link to Canvas. **Your Challenge submission is not complete without all files uploaded to GitHub.**


# Setup
<!-- ----------------------------------------------------------------------- -->

```{r setup}
library(tidyverse)
```

### __q1__ Load the population data from c06; simply replace `filename_pop` below.

```{r q1-task}
## TODO: Give the filename for your copy of Table B01003
filename_pop <- "c06-data/ACSDT5Y2018.B01003-Data.csv"

## NOTE: No need to edit
df_pop <-
  read_csv(
    filename_pop,
    skip = 1,
  ) %>% 
  rename(
    population_estimate = `Estimate!!Total`
  )

df_pop
```

You might wonder why the `Margin of Error` in the population estimates is listed as `*****`. From the [documentation (PDF link)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj81Omy16TrAhXsguAKHTzKDQEQFjABegQIBxAB&url=https%3A%2F%2Fwww2.census.gov%2Fprograms-surveys%2Facs%2Ftech_docs%2Faccuracy%2FMultiyearACSAccuracyofData2018.pdf%3F&usg=AOvVaw2TOrVuBDlkDI2gde6ugce_) for the ACS:

> If the margin of error is displayed as ‘*****’ (five asterisks), the estimate has been controlled to be equal to a fixed value and so it has no sampling error. A standard error of zero should be used for these controlled estimates when completing calculations, such as those in the following section.

This means that for cases listed as `*****` the US Census Bureau recommends treating the margin of error (and thus standard error) as zero.

### __q2__ Obtain median income data from the Census Bureau:

- `Filter > Topics > Income and Poverty > Income and Poverty`
- `Filter > Geography > County > All counties in United States`
- Look for `Median Income in the Past 12 Months` (Table S1903)
- Download the 2018 5-year ACS estimates; save to your `data` folder and add the filename below.

```{r q2-task}
## TODO: Give the filename for your copy of Table S1903
filename_income <- "data/ACSST5Y2018.S1903-Data.csv"

## NOTE: No need to edit
df_income <-
  read_csv(filename_income, skip = 1)
df_income
```

Use the following test to check that you downloaded the correct file:

```{r q2-tests}
## NOTE: No need to edit, use to check you got the right file.
assertthat::assert_that(
  df_income %>%
    filter(Geography == "0500000US01001") %>%
    pull(`Estimate!!Percent Distribution!!FAMILY INCOME BY FAMILY SIZE!!2-person families`)
  == 45.6
)

print("Well done!")
```


This dataset is in desperate need of some *tidying*. To simplify the task, we'll start by considering the `\\d-person families` columns first.

### __q3__ Tidy the `df_income` dataset by completing the code below. Pivot and rename the columns to arrive at the column names `id, geographic_area_name, category, income_estimate, income_moe`.

*Hint*: You can do this in a single pivot using the `".value"` argument and a `names_pattern` using capture groups `"()"`. Remember that you can use an OR operator `|` in a regex to allow for multiple possibilities in a capture group, for example `"(Estimate|Margin of Error)"`.

```{r q3-task}
df_q3 <-
  df_income %>%
  select(
    Geography,
    contains("Geographic"),
    # This will select only the numeric d-person family columns;
    # it will ignore the annotation columns
    contains("median") & matches("\\d-person families") & !contains("Annotation of")
  ) %>%
  mutate(across(contains("median"), as.numeric)) %>%
  pivot_longer(
    cols = !"Geography" & !"Geographic Area Name",
    names_to = c(".value", "category"),
    names_pattern = "(Estimate|Margin of Error).+([0-9].+)",
    values_drop_na = TRUE
  ) %>%
  rename(
    geographic_area_name = "Geographic Area Name",
    income_estimate = "Estimate",
    income_moe = "Margin of Error"
  ) %>%
  glimpse()

df_q3
```

Use the following tests to check your work:

```{r q3-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q3),
  c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe")
))

assertthat::assert_that(
  df_q3 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_moe)
  == 6663
)

print("Nice!")
```


The data gives finite values for the Margin of Error, which is closely related to the Standard Error. The Census Bureau documentation gives the following relationship between Margin of Error and Standard Error:

$$\text{MOE} = 1.645 \times \text{SE}.$$

### __q4__ Convert the margin of error to standard error. Additionally, compute a 99% confidence interval on income, and normalize the standard error to `income_CV = income_SE / income_estimate`. Provide these columns with the names `income_SE, income_lo, income_hi, income_CV`.

```{r q4-task}

q99 <- qnorm( 1 - (1 - 0.99) / 2 )
# don't calculate mean (income_estimate) because income_estimate is already a statistic 

df_q4 <-
  df_q3 %>%
    mutate(
      income_SE = income_moe / 1.645,
      income_CV = income_SE / income_estimate,
      income_lo = income_estimate - q99 * income_SE,
      income_hi = income_estimate + q99 * income_SE
    )

df_q4
```

Use the following tests to check your work:

```{r q4-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q4),
  c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe",
    "income_SE", "income_lo", "income_hi", "income_CV")
))

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_SE) - 4050.456
  ) / 4050.456 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_lo) - 54513.72
  ) / 54513.72 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_hi) - 75380.28
  ) / 75380.28 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_CV) - 0.06236556
  ) / 0.06236556 < 1e-3
)

print("Nice!")
```


One last wrangling step: We need to join the two datasets so we can compare population with income.

### __q5__ Join `df_q4` and `df_pop`.

```{r q5-task}
## TODO: Join df_q4 and df_pop by the appropriate column

df_data <-
  df_q4 %>%
    left_join(df_pop, by = "Geography")

df_data
```

# Analysis
<!-- ----------------------------------------------------------------------- -->

We now have both estimates and confidence intervals for `\\d-person families`. Now we can compare cases with quantified uncertainties: Let's practice!

### __q6__ Study the following graph, making sure to note what you can *and can't* conclude based on the estimates and confidence intervals. Document your observations below and answer the questions.

```{r q6-task}
## NOTE: No need to edit; run and inspect
wid <- 0.5

df_data %>%
  filter(str_detect(geographic_area_name, "Massachusetts")) %>%
  mutate(
    county = str_remove(geographic_area_name, " County,.*$"),
    county = fct_reorder(county, income_estimate)
  ) %>%

  ggplot(aes(county, income_estimate, color = category)) +
  geom_errorbar(
    aes(ymin = income_lo, ymax = income_hi),
    position = position_dodge(width = wid)
  ) +
  geom_point(position = position_dodge(width = wid)) +

  coord_flip() +
  labs(
    x = "County",
    y = "Median Household Income"
  )
```

**Observations**:

- Document your observations here.
  - In general, the larger the family, the greater the median household income.
  - The confidence intervals tend to overlap more as family size increases, suggesting that additional household members does not necessarily correspond to greater median incomes.
  - The median household income in Suffolk County, which includes the only major metropolitan area in Massachusetts (Boston) is lower for 2 person families than more suburban counties like Norfolk County. However, this is just an observation and we can't make generalizations based on this data alone about incomes for suburban vs. urban areas.
  - In general, the confidence intervals tend to be wider for larger families, showing that there is a greater range of potential incomes for wider families.
- Can you confidently distinguish between household incomes in Suffolk county? Why or why not?
  - No, we can't confidently distinguish between household incomes in Suffolk county because the confidence intervals by category overlap with each other.
- Which counties have the widest confidence intervals?
  - Nantucket, Hampshire, Dukes, and Berkshire have the widest confidence intervals of the counties in Massachusetts.

In the next task you'll investigate the relationship between population and uncertainty.

### __q7__ Plot the standard error against population for all counties. Create a visual that effectively highlights the trends in the data. Answer the questions under *observations* below.

*Hint*: Remember that standard error is a function of *both* variability (e.g. variance) and sample size.

```{r q7-task}
df_data %>%
  ggplot(mapping = aes(x = population_estimate, y = income_SE)) +
    geom_point() +
    scale_x_continuous(trans='log10') +
    scale_y_continuous(trans='log10')

```

**Observations**:

- What *overall* trend do you see between `SE` and population? Why might this trend exist?
  - In general, higher population means higher standard error. Since standard error is also a function of sample size, we would expect standard error to decrease as sample size increases. Counties with larger populations should have more data points (i.e. a greater sample size), and therefore a smaller standard error than counties with smaller populations.
- What does this *overall* trend tell you about the relative ease of studying small vs large counties?
  - Since large counties have more data points, the standard error for large counties should generally be smaller, making it easier to make more confident observations and draw conclusions.

# Going Further
<!-- ----------------------------------------------------------------------- -->

Now it's your turn! You have income data for every county in the United States: Pose your own question and try to answer it with the data.

### __q8__ Pose your own question about the data. Create a visualization (or table) here, and document your observations.


```{r q8-task}
# redo data pivoting and selecting to get incomes by race

df_q8 <-
  df_income %>%
  select(
    Geography,
    contains("Geographic"),
    # This will select only the numeric d-person family columns;
    # it will ignore the annotation columns
    contains("median") & matches("\\One race--!!") & !contains("Annotation of")
  ) %>%
  mutate(across(contains("median"), as.numeric)) %>%
  pivot_longer(
    cols = !"Geography" & !"Geographic Area Name",
    names_to = c(".value", "race"),
    names_pattern = "(Estimate|Margin of Error).+One race--!!(.+)",
    values_drop_na = TRUE
  ) %>%
  rename(
    geographic_area_name = "Geographic Area Name",
    income_estimate = "Estimate",
    income_moe = "Margin of Error"
  ) %>%
  mutate(
      income_SE = income_moe / 1.645,
      income_CV = income_SE / income_estimate,
      income_lo = income_estimate - q99 * income_SE,
      income_hi = income_estimate + q99 * income_SE
    ) %>%
  drop_na()

df_q8
```


```{r}
# From race dataset get % black to filter for majority black counties
# Load Race dataset for 2021
df_race <- read_csv("c06-data/ACSDT1y2021.B02001-Data.csv", skip = 1, col_select = c("Geography", "Geographic Area Name", "Estimate!!Total:", "Estimate!!Total:!!Black or African American alone", "Estimate!!Total:!!Asian alone")) %>%
  select(c(
    "Geography", 
    "Geographic Area Name",
    population = "Estimate!!Total:",
    population_asian = "Estimate!!Total:!!Asian alone",
    population_black = "Estimate!!Total:!!Black or African American alone")) %>%
  mutate_at(c('population','population_black', 'population_asian'), as.numeric)  %>%
  mutate(pct_black = population_black/population, pct_asian = population_asian/population) %>%
  rename(
    geographic_area_name = "Geographic Area Name"
  )

df_q8_2 <-
  df_q8 %>%
  left_join(df_race, by = "geographic_area_name") %>%
  select(geographic_area_name, income_estimate, income_lo, income_hi, race, pct_black) %>%
  drop_na()

df_q8_2

```

```{r}
df_q8_2 %>%
  filter(pct_black > 0.5, race == "Asian" | race == "Black or African American") %>%
  ggplot(aes(geographic_area_name, income_estimate, color = race)) +
  geom_errorbar(
    aes(ymin = income_lo, ymax = income_hi),
    position = position_dodge(width = wid)
  ) +
  geom_point(position = position_dodge(width = wid)) +
  coord_flip() +
  labs(
    x = "County",
    y = "Median Household Income for Majority-Black counties"
  )

```




**Observations**:

Question: In majority black counties, how does median income of Asian households compare with Black or African American households?

Context: In my Race in American Politics class last year at Wellesley, we read _Bitter Fruit: The Politics of Black-Korean Conflict in New York City_ by Claire Jean Kim. Kim establishes the framework of racial triangulation, where Asians are "weaponized" by white people against other minorities - particularly Black people - in order to depress solidarity amongst people of color that could threaten white dominance. Valorization of Asians relative to Blacks allows whites to claim that assumed "cultural values", not discrimination and immigration laws selecting for wealthier Asians, are why Asians (particularly East Asians) have in general achieved economic success. As Kim explains, residential segregation has left Black people without the resources to take advantage of private-sector niches opened by white flight. On the other hand, the Hart-Celler Act precipitated the immigration of educated, middle-class Asians with the necessary capital for retail store ownership. These immigrants largely found success by opening stores—but not living in—Black neighborhoods. In the Black nationalist frame of community control, not only have Black people been closed off from small-business ownership in their own neighborhoods, but money spent by Black customers in these stores leave the community (i.e. are going to Asian households). _Bitter Fruit_ documents and analyzes the conflicts that often result from this juxtaposition. After reading _Bitter Fruit_, I was interested in comparing the median household incomes for Asian and Black households in majority black counties to provide additional data to the community control framework.

- The confidence interval for Asian household incomes in majority black counties is in general wider than Black households.
- The Asian household income _estimate_ is greater than the median Black household income in every majority black county.
- For several counties the confidence interval for Asian household income encompasses the confidence interval for Black household income, so we cannot confidently conclude that the household income for Asian households is greater than that of Black households for counties such as Jefferson and Portsmouth City, Virginia, but we can for Shelby countyand Dekalb county.


# References
<!-- ----------------------------------------------------------------------- -->

[1] Spielman SE, Folch DC, Nagle NN (2014) Patterns and causes of uncertainty in the American Community Survey. Applied Geography 46: 147–157. pmid:25404783 [link](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNqZ6Y2IzszmGgIGH4JAPzQN0)
